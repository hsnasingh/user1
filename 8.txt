Assignment 8   (matrix multiplication):



from pyspark.sql import SparkSession

# Start Spark
spark = SparkSession.builder.appName("MatrixMul").getOrCreate()
sc = spark.sparkContext

# Input matrices (A: 2x3, B: 3x2)
A = [(0, 0, 4), (0, 1, 6), (0, 2, 8),
     (1, 0, 5), (1, 1, 5), (1, 2, 4)]

B = [(0, 0, 7), (0, 1, 8),
     (1, 0, 9), (1, 1, 10),
     (2, 0, 11), (2, 1, 12)]

# Parallelize
rddA = sc.parallelize(A)
rddB = sc.parallelize(B)

# Map and join
a_mapped = rddA.map(lambda x: (x[1], (x[0], x[2])))
b_mapped = rddB.map(lambda x: (x[0], (x[1], x[2])))
joined = a_mapped.join(b_mapped)

# Multiply and reduce
products = joined.map(lambda x: ((x[1][0][0], x[1][1][0]), x[1][0][1] * x[1][1][1]))
result = products.reduceByKey(lambda x, y: x + y)

# Show result
for ((i, j), val) in sorted(result.collect()):
    print(f"({i}, {j}) -> {val}")

spark.stop()






Explanation:


This PySpark code performs **matrix multiplication** using **RDDs**. Letâ€™s walk through it step-by-step with detailed explanation and visuals.

---

### ðŸ”· **Overview**

- Matrix A is **2Ã—3**
- Matrix B is **3Ã—2**
- The result of `A Ã— B` is a **2Ã—2** matrix

---

### ðŸ”¶ **1. Start Spark Session**
```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("MatrixMul").getOrCreate()
sc = spark.sparkContext
```
- Starts a Spark session named **"MatrixMul"**
- Gets the Spark **context** (for working with RDDs)

---

### ðŸ”¶ **2. Input Matrices**
```python
A = [(0, 0, 4), (0, 1, 6), (0, 2, 8),
     (1, 0, 5), (1, 1, 5), (1, 2, 4)]

B = [(0, 0, 7), (0, 1, 8),
     (1, 0, 9), (1, 1, 10),
     (2, 0, 11), (2, 1, 12)]
```

Each tuple is in the form: `(row_index, col_index, value)`

So:
```
Matrix A =
[[4, 6, 8],
 [5, 5, 4]]

Matrix B =
[[ 7,  8],
 [ 9, 10],
 [11, 12]]
```

---

### ðŸ”¶ **3. Parallelize to RDDs**
```python
rddA = sc.parallelize(A)
rddB = sc.parallelize(B)
```
- Converts Python lists to **distributed RDDs**

---

### ðŸ”¶ **4. Map to Prepare for Join**
```python
a_mapped = rddA.map(lambda x: (x[1], (x[0], x[2])))
b_mapped = rddB.map(lambda x: (x[0], (x[1], x[2])))
```
Weâ€™re **mapping by the common dimension** (`k`) for matrix multiplication:

- `a_mapped`: keyed by **Aâ€™s column** â†’ `(k, (i, A[i][k]))`
- `b_mapped`: keyed by **Bâ€™s row** â†’ `(k, (j, B[k][j]))`

Why? Because we multiply `A[i][k] * B[k][j]`, and `k` is common in both.

---

### ðŸ”¶ **5. Join on the Shared Index `k`**
```python
joined = a_mapped.join(b_mapped)
```
- Joins A and B entries with the **same `k`**
- Output is: `(k, ((i, A_val), (j, B_val)))`

---

### ðŸ”¶ **6. Multiply Corresponding Values**
```python
products = joined.map(lambda x: ((x[1][0][0], x[1][1][0]), x[1][0][1] * x[1][1][1]))
```
- Computes: `A[i][k] * B[k][j]`
- Key becomes: `(i, j)` for result matrix `C[i][j]`
- Value: product of the two numbers

---

### ðŸ”¶ **7. Reduce to Sum Partial Products**
```python
result = products.reduceByKey(lambda x, y: x + y)
```
- Adds all the partial products for each `(i, j)`
- Final value is `C[i][j]`

---

### ðŸ”¶ **8. Display Results**
```python
for ((i, j), val) in sorted(result.collect()):
    print(f"({i}, {j}) -> {val}")
```
- Collects results to the driver and prints them in order.

---

### âœ… **Expected Output**
For the input matrices A (2Ã—3) and B (3Ã—2), result C (2Ã—2):

```
C[0][0] = 4Ã—7 + 6Ã—9 + 8Ã—11 = 28 + 54 + 88 = 170
C[0][1] = 4Ã—8 + 6Ã—10 + 8Ã—12 = 32 + 60 + 96 = 188
C[1][0] = 5Ã—7 + 5Ã—9 + 4Ã—11 = 35 + 45 + 44 = 124
C[1][1] = 5Ã—8 + 5Ã—10 + 4Ã—12 = 40 + 50 + 48 = 138
```

So output:
```
(0, 0) -> 170
(0, 1) -> 188
(1, 0) -> 124
(1, 1) -> 138
```

---


